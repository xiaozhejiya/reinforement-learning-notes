### 马尔可夫状态

$$
信息状态(也叫马尔可夫状态)包含历史上所有有用的信息。
\\状态S_t具有马尔可夫性,当且仅当\mathbb{P}[S_{t+1}|S_t]=\mathbb{P}[S_{t+1}|S_1,\cdots,S_t]
\\给定当前时刻的状态,将来与历史无关
$$

### 完全可观测的环境

完全可观测: 智能体可以直接观察到全部环境状态
$$
S_t=S_t^a=S_t^e
$$
信息状态=智能体状态=环境状态

正式地说,这是马尔科夫决策过程(MDP)

### 部分可观测的环境

部分可观测: 智能体可以观测到环境的部分

比如:打麻将 斗地主

智能体状态不等于环境状态

正式地说,这是部分可观测马尔可夫决策过程(POMDP)

### 策略

策略是学习智能体在特定时间的行为方式

- 是从状态到行动的映射
- 确定性策略:$a=\pi(s)$
- 随机策略: $\pi(a|s)=P(A_t=a|S_t=s)$

### 价值函数

价值函数是对为了累积奖励的预测

- 用于评估在给定策略下状态的好坏
- 可用于选择动作

$$
V_\pi(s)=\mathbb{E}_\pi[R_{t+1}+\gamma R_{r+2}+\gamma^2 R_{t+3}+\cdots|S_t=s]
$$

### 模型

- 模型用于模拟环境的行为，建模环境的动态特性

- 解决下述两个问题:

  - 状态转移概率:用来预测环境的下一个状态
    $$
    P_{ss'}^a=\mathbb{P}[S_{t+1}=s'|S_t=s,A_t=a]
    $$

  - 奖励: 预测环境给出的下一个即使奖励
    $$
    R_{s}^a=\mathbb{E}[R_{t+1}|S_t=s,A_t=a]
    $$

- 环境真实的运行机制通常不称为模型，而称为环境动力学

- 模型并不能立即给我们一个好的策略

## 智能体分类

### 基于策略的更新与学习方向

基于策略的更新与学习方向，强化学习方法可以分为:

- 基于价值函数
- 基于直接策略探索
- 基于执行者-评论者(Actor-Critic)

### 根据强化学习算法是否依赖模型

根据强化学习算法是否依赖模型，强化学习方法可以分为:

- 基于模型的强化学习算法
- 无模型的强化学习算法

### 根据环境返回的回报函数是否已知

根据环境返回的回报函数是否已知，强化学习方法可以分为:

- 正向强化学习算法
- 逆向强化学习算法：从专家的示例中学习回报函数(智能驾驶)



## 强化学习问题

### 学习(Learning)与规划(Planning)

序列决策中的两个基础问题:

- 强化学习
  - 环境初始未知
  - 智能体不断与环境交互 
  - 智能体提升它的策略
- 规划
  - 环境模型已知
  - 智能体根据Model进行计算(不进行额外的交互)
  - 智能体提升它的策略

### 探索(Exploration)和利用(Exploitation)

- 强化学习类似于一个试错的学习
- 智能体从其于环境的交互中发现一个好的策略
- 在试错的过程中不会损失太多奖励
- 探索会发现有关环境的更多信息，有选择地放弃一些奖励
- 利用已知信息来最大化回报，强调开发利用已有信息
- 探索和利用是决策时需要平衡地两个方面

### 预测和控制(Prediction)与控制(Control)

- 预测:估计未来(评估)
  - 策略已经给定
- 控制:最大化未来(优化)
  - 找到最优策略